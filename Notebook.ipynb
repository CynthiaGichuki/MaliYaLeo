{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b46e33",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f2982",
   "metadata": {},
   "source": [
    "1. Why LSTM could work here\n",
    "- LSTMs are neural networks designed for time-series, capable of remembering past states (lags) automatically.\n",
    "\n",
    "- Can capture nonlinear patterns, spikes, and delayed effects better than Prophet.\n",
    "\n",
    "- Handles multi-step forecasting by learning sequential dependencies.\n",
    "\n",
    "2. Basic Setup\n",
    "We’ll:\n",
    "\n",
    "1. Use one commodity at a time (e.g., Beans Wholesale first).\n",
    "\n",
    "2. Scale prices (LSTMs are sensitive to scale).\n",
    "\n",
    "3. Create sequences (lags):\n",
    "\n",
    " - Input: last n days (e.g., 30)\n",
    "\n",
    " - Output: next day’s price.\n",
    "\n",
    "4. Train an LSTM model using Keras/TensorFlow.\n",
    "\n",
    "5. Evaluate on last 60 days using RMSE, MAE, MAPE, R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9b218f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select single series (Beans Wholesale)\n",
    "commodity = \"Beans\"\n",
    "price_col = \"WholesaleUnitPrice\"\n",
    "df_series = df_p[df_p[\"Commodity\"] == commodity][[\"Date\", price_col]].dropna().sort_values(\"Date\")\n",
    "\n",
    "# Scale prices (LSTM needs scaled inputs)\n",
    "scaler = MinMaxScaler()\n",
    "prices_scaled = scaler.fit_transform(df_series[[price_col]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6847d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences (e.g., 30 days input → 1 day output)\n",
    "def create_sequences(data, window=30):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(data)):\n",
    "        X.append(data[i-window:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 30\n",
    "X, y = create_sequences(prices_scaled, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6425fd2",
   "metadata": {},
   "source": [
    "SPLIT TRAIN/TEST DATA FOR THE LAST 60 DAYS AND RESHAPE FOR LSTM SAMPLES,TIMESTEPS AND ALSO FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57700291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test (last 60 days as test)\n",
    "split = len(X) - 60\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6a352",
   "metadata": {},
   "source": [
    "### Build LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8cfb79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 0.0307\n",
      "Epoch 2/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0219\n",
      "Epoch 3/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.0217\n",
      "Epoch 4/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0214\n",
      "Epoch 5/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0200\n",
      "Epoch 6/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 7/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 8/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0207\n",
      "Epoch 9/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0209\n",
      "Epoch 10/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0204\n",
      "Epoch 11/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0204\n",
      "Epoch 12/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0200\n",
      "Epoch 13/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0200\n",
      "Epoch 14/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0200\n",
      "Epoch 15/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 0.0199\n",
      "Epoch 16/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0208\n",
      "Epoch 17/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0201\n",
      "Epoch 18/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 19/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0196\n",
      "Epoch 20/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 21/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0200\n",
      "Epoch 22/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 23/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0201\n",
      "Epoch 24/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0204\n",
      "Epoch 25/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0201\n",
      "Epoch 26/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0203\n",
      "Epoch 27/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 28/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0202\n",
      "Epoch 29/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0204\n",
      "Epoch 30/30\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x169962f90d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(window_size, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # Predict next price\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=16, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7d650",
   "metadata": {},
   "source": [
    "### Prediction and evalutuation of the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07a0207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step\n",
      "LSTM Results for Beans (WholesaleUnitPrice):\n",
      "MAE: 23.20, RMSE: 27.96, R²: 0.0028, MAPE: 19.45%\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred_scaled = model.predict(X_test)\n",
    "pred_prices = scaler.inverse_transform(pred_scaled.reshape(-1, 1))\n",
    "actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "rmse = mean_squared_error(actual_prices, pred_prices, squared=False)\n",
    "r2 = r2_score(actual_prices, pred_prices)\n",
    "mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100\n",
    "\n",
    "print(f\"LSTM Results for {commodity} ({price_col}):\")\n",
    "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}, MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b98d7c",
   "metadata": {},
   "source": [
    "THE INTERPRETATION OF THE MODEL:\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d689a",
   "metadata": {},
   "source": [
    "### HYPERTUNE THE MODEL DUE TO THE POOR PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1af26453",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preparation and rollng of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8867a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity = \"Beans\"\n",
    "price_col = \"WholesaleUnitPrice\"\n",
    "\n",
    "df_series = df_p[df_p[\"Commodity\"] == commodity][[\"Date\", price_col]].dropna().sort_values(\"Date\")\n",
    "df_series.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Add rolling features\n",
    "df_series[\"rolling_mean_7\"] = df_series[price_col].rolling(7).mean()\n",
    "df_series[\"rolling_std_7\"] = df_series[price_col].rolling(7).std()\n",
    "df_series = df_series.dropna()  # Drop first 7 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4210dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale features\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_series)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled, index=df_series.index, columns=df_series.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305a106",
   "metadata": {},
   "source": [
    "### Creation of sequence (30 days input -> 7 days forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3505885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_multi(data, window=30, horizon=7):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(data) - horizon):\n",
    "        X.append(data[i-window:i])\n",
    "        y.append(data[i:i+horizon, 0])  # First column is price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 30\n",
    "horizon = 7\n",
    "X, y = create_sequences_multi(scaled_df.values, window_size, horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c615307",
   "metadata": {},
   "source": [
    "### Train/test split (last 60 days reserved for testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50da5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = len(X) - 60\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b0ce4",
   "metadata": {},
   "source": [
    "#### LSTM Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49e25b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 33ms/step - loss: 0.0357 - val_loss: 0.0222\n",
      "Epoch 2/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - loss: 0.0221 - val_loss: 0.0226\n",
      "Epoch 3/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 32ms/step - loss: 0.0213 - val_loss: 0.0218\n",
      "Epoch 4/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - loss: 0.0211 - val_loss: 0.0226\n",
      "Epoch 5/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 0.0207 - val_loss: 0.0221\n",
      "Epoch 6/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 31ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 7/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 0.0205 - val_loss: 0.0219\n",
      "Epoch 8/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 9/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 0.0202 - val_loss: 0.0221\n",
      "Epoch 10/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 11/50\n",
      "\u001b[1m860/860\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - loss: 0.0202 - val_loss: 0.0222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1699555ae10>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(window_size, X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(horizon))  # Predict next 7 prices\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2,\n",
    "          callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434d16f",
   "metadata": {},
   "source": [
    "### Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74d87114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_scaled = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30940328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform predictions and actuals (only price column)\n",
    "pred_full = np.zeros((pred_scaled.shape[0], scaled_df.shape[1]))\n",
    "actual_full = np.zeros_like(pred_full)\n",
    "\n",
    "pred_full[:, 0] = pred_scaled[:, -1]  # Compare last predicted day\n",
    "actual_full[:, 0] = y_test[:, -1]     # Compare last actual day\n",
    "\n",
    "pred_prices = scaler.inverse_transform(pred_full)[:, 0]\n",
    "actual_prices = scaler.inverse_transform(actual_full)[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52306620",
   "metadata": {},
   "source": [
    "#### Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f13beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LSTM Results for Beans (WholesaleUnitPrice):\n",
      "MAE: 23.73, RMSE: 28.18, R²: -0.0062, MAPE: 20.44%\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "rmse = mean_squared_error(actual_prices, pred_prices, squared=False)\n",
    "r2 = r2_score(actual_prices, pred_prices)\n",
    "mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100\n",
    "\n",
    "print(f\"Tuned LSTM Results for {commodity} ({price_col}):\")\n",
    "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}, MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fdcfc",
   "metadata": {},
   "source": [
    "What this does differently:\n",
    "\n",
    "- Uses all commodities together (more data = better learning).\n",
    "\n",
    "- Feeds lag, rolling, and seasonal features like we used for XGBoost.\n",
    "\n",
    "- Adds a Conv1D front-end (captures local shocks) + stacked LSTM for long trends.\n",
    "\n",
    "- Predicts 7 days ahead, not just 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c80b65",
   "metadata": {},
   "source": [
    "### Prepare Features for All Commodities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29e52349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = df_p.copy().dropna(subset=[\"WholesaleUnitPrice\"])  # Focus on wholesale for now\n",
    "df_mc = df_mc.sort_values(\"Date\")\n",
    "\n",
    "# Lag features\n",
    "df_mc[\"Wholesale_t-1\"] = df_mc.groupby(\"Commodity\")[\"WholesaleUnitPrice\"].shift(1)\n",
    "df_mc[\"Wholesale_t-7\"] = df_mc.groupby(\"Commodity\")[\"WholesaleUnitPrice\"].shift(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04464ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling stats\n",
    "df_mc[\"RollingMean_7\"] = df_mc.groupby(\"Commodity\")[\"WholesaleUnitPrice\"].transform(lambda x: x.rolling(7).mean())\n",
    "df_mc[\"RollingMean_14\"] = df_mc.groupby(\"Commodity\")[\"WholesaleUnitPrice\"].transform(lambda x: x.rolling(14).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f65b6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonality (encode day of week as sin/cos for cyclicity)\n",
    "df_mc[\"Date\"] = pd.to_datetime(df_mc[\"Date\"], errors=\"coerce\")\n",
    "df_mc[\"dayofweek\"] = df_mc[\"Date\"].dt.dayofweek\n",
    "df_mc[\"dow_sin\"] = np.sin(2 * np.pi * df_mc[\"dayofweek\"] / 7)\n",
    "df_mc[\"dow_cos\"] = np.cos(2 * np.pi * df_mc[\"dayofweek\"] / 7)\n",
    "\n",
    "# Dropping NA rows (from lag/rolling)\n",
    "df_mc = df_mc.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80d67772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Selecting final features\n",
    "features = [\"WholesaleUnitPrice\", \"Wholesale_t-1\", \"Wholesale_t-7\",\n",
    "            \"RollingMean_7\", \"RollingMean_14\", \"dow_sin\", \"dow_cos\"]\n",
    "target_col = \"WholesaleUnitPrice\"\n",
    "\n",
    "# Scale per commodity (fit per commodity for fairness)\n",
    "scaled_data = []\n",
    "scalers = {}\n",
    "\n",
    "for commodity, group in df_mc.groupby(\"Commodity\"):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_group = scaler.fit_transform(group[features])\n",
    "    scaled_data.append(pd.DataFrame(scaled_group, columns=features, index=group.index))\n",
    "    scalers[commodity] = scaler\n",
    "\n",
    "scaled_df = pd.concat(scaled_data).sort_index()\n",
    "df_mc[features] = scaled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080115f",
   "metadata": {},
   "source": [
    "### Build sequences (30-day input -> 7-day forecast) -----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2349dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_multivariate(data, window=30, horizon=7, target_idx=0):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(data) - horizon):\n",
    "        X.append(data[i-window:i])\n",
    "        y.append(data[i:i+horizon, target_idx])  # Target is first feature (price)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build per commodity sequences and combine\n",
    "all_X, all_y = [], []\n",
    "for commodity in df_mc[\"Commodity\"].unique():\n",
    "    group = df_mc[df_mc[\"Commodity\"] == commodity][features].values\n",
    "    # Skip commodities with too few data points\n",
    "    if len(group) < 37:  # 30-day window + 7-day horizon\n",
    "        continue\n",
    "    \n",
    "    X_c, y_c = create_sequences_multivariate(group)\n",
    "    # Only keep non-empty results\n",
    "    if len(X_c) > 0:\n",
    "        all_X.append(X_c)\n",
    "        all_y.append(y_c)\n",
    "\n",
    "# Concatenate all valid sequences\n",
    "X = np.concatenate(all_X)\n",
    "y = np.concatenate(all_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0311bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (last 10% as test)\n",
    "split = int(len(X) * 0.9)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2a28559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 23ms/step - loss: 0.0259 - val_loss: 0.0133\n",
      "Epoch 2/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - loss: 0.0183 - val_loss: 0.0141\n",
      "Epoch 3/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step - loss: 0.0179 - val_loss: 0.0133\n",
      "Epoch 4/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - loss: 0.0177 - val_loss: 0.0129\n",
      "Epoch 5/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step - loss: 0.0176 - val_loss: 0.0131\n",
      "Epoch 6/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 21ms/step - loss: 0.0175 - val_loss: 0.0131\n",
      "Epoch 7/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step - loss: 0.0175 - val_loss: 0.0132\n",
      "Epoch 8/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step - loss: 0.0173 - val_loss: 0.0130\n",
      "Epoch 9/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 26ms/step - loss: 0.0173 - val_loss: 0.0130\n",
      "Epoch 10/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 30ms/step - loss: 0.0173 - val_loss: 0.0130\n",
      "Epoch 11/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 32ms/step - loss: 0.0173 - val_loss: 0.0129\n",
      "Epoch 12/50\n",
      "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 32ms/step - loss: 0.0174 - val_loss: 0.0130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1698f124c90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Conv1D + LSTM Model \n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(horizon))  # 7-day forecast\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2,\n",
    "          callbacks=[early_stop], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57633b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016993F7F4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict \n",
    "pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Only evaluate last predicted day for simplicity (can extend)\n",
    "pred_last_day = pred_scaled[:, -1]\n",
    "actual_last_day = y_test[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89e58c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Commodity Conv1D+LSTM Results (Wholesale Prices):\n",
      "MAE: 14.82, RMSE: 22.20, R²: 0.5850, MAPE: 370.87%\n"
     ]
    }
   ],
   "source": [
    "# Inverse scaling for evaluation (average scaler since commodities differ)\n",
    "# Simplify by rescaling using global min/max of all commodities (approximation)\n",
    "global_scaler = MinMaxScaler()\n",
    "global_scaler.fit(df_p[[\"WholesaleUnitPrice\"]])  # Fit on raw prices\n",
    "pred_prices = global_scaler.inverse_transform(pred_last_day.reshape(-1, 1)).flatten()\n",
    "actual_prices = global_scaler.inverse_transform(actual_last_day.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "rmse = mean_squared_error(actual_prices, pred_prices, squared=False)\n",
    "r2 = r2_score(actual_prices, pred_prices)\n",
    "mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100\n",
    "\n",
    "print(f\"Multi-Commodity Conv1D+LSTM Results (Wholesale Prices):\")\n",
    "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}, MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2f6cf",
   "metadata": {},
   "source": [
    "The LSTM is still struggling despite the Conv1D + seasonal features:\n",
    "\n",
    "R² = 0.5882 → better than single-series LSTM (which was near 0) but still far below XGBoost (0.91+).\n",
    "\n",
    "MAE and RMSE are acceptable, but the MAPE (356%) is absurdly high because many wholesale prices are close to zero (even 0.5), so percentage errors explode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0e388-0058-4789-89ac-ea503f5f5c26",
   "metadata": {},
   "source": [
    "### CONCLUSION\n",
    "XGBoost predicts the base price trend (it’s already very strong: R² ≈ 0.92)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
